{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4_oKcTi9cIr",
        "colab_type": "text"
      },
      "source": [
        "Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U18-lLvKDFH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa7e71a9-843a-4e0e-ef5b-78c6a0bffdb4",
        "id": "hM4hmL2I87sF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "OUTPUT_DIR = 'OUTPUT_MODEL'\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: OUTPUT_TEST *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udbcSm3Q9gJh",
        "colab_type": "text"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ao7es7ESDpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datanews=pd.read_excel(r'path_to_newscontent.xlsx','Sheet1')\n",
        "datacomment=pd.read_excel(r'path_to_comments.xlsx','Sheet1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnTIR6kWDOTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datapreparation(data,data_column):\n",
        "  data[data_column]=[str(i) for i in data[data_column]]\n",
        "  num_split=int(len(data.index)*0.8)\n",
        "  return data, data_column, num_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fqxXFsl5cyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0, 1, 2]\n",
        "data, DATA_COLUMN, num_split= datapreparation(datacomment, \"comment\")\n",
        "#data, DATA_COLUMN, num_split= datapreparation(datanews, \"all_lower\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2-XOMYiDQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_InputExamples = data.iloc[:num_split].apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
        "                                                                 text_a = x[DATA_COLUMN], \n",
        "                                                                 text_b = None, \n",
        "                                                                 label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = data.iloc[(num_split+1):].apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                 text_a = x[DATA_COLUMN], \n",
        "                                                                 text_b = None, \n",
        "                                                                 label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKKTJyLgDRzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bXE6hxL7o08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnKDPOEO9joZ",
        "colab_type": "text"
      },
      "source": [
        "BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foz60worDVXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "    \n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    \n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    \n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    \n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    \n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F38uOuEQDaTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params): \n",
        "\n",
        "  input_ids = features[\"input_ids\"]\n",
        "  input_mask = features[\"input_mask\"]\n",
        "  segment_ids = features[\"segment_ids\"]\n",
        "  label_ids = features[\"label_ids\"]\n",
        "  is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "  if not is_predicting:\n",
        "\n",
        "    (loss, predicted_labels, log_probs) = create_model(\n",
        "      is_predicting, input_ids, input_mask, segment_ids, label_ids, 3)\n",
        "\n",
        "    train_op = bert.optimization.create_optimizer(\n",
        "        loss, params[\"learning_rate\"], \n",
        "        params[\"num_train_steps\"], params[\"num_warmup_steps\"], use_tpu=False)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      return tf.estimator.EstimatorSpec(mode=mode,\n",
        "        loss=loss,\n",
        "        train_op=train_op)\n",
        "  else:\n",
        "    (predicted_labels, log_probs) = create_model(\n",
        "      is_predicting, input_ids, input_mask, segment_ids, label_ids, 3)\n",
        "\n",
        "    predictions = {\n",
        "        'probabilities': log_probs,\n",
        "        'labels': predicted_labels,\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkaaWto-9nrx",
        "colab_type": "text"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQcjnQYkDcd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_TRAIN_EPOCHS = 100\n",
        "WARMUP_PROPORTION = 0.1\n",
        "SAVE_CHECKPOINTS_STEPS = 100\n",
        "SAVE_SUMMARY_STEPS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fwsWqZ2FFeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    log_step_count_steps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK43V9urFGKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IqrnWBx1Ces",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "  \n",
        "  def input_fn(params):\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32)\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj6Ix3272b-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_fn = input_fn_builder(\n",
        "  features=train_features,\n",
        "  seq_length=MAX_SEQ_LENGTH,\n",
        "  is_training=True,\n",
        "  drop_remainder=False)\n",
        "test_input_fn = input_fn_builder(\n",
        "  features=test_features,\n",
        "  seq_length=MAX_SEQ_LENGTH,\n",
        "  is_training=False,\n",
        "  drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9W4djWZt4mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE,\n",
        "              \"learning_rate\": LEARNING_RATE,\n",
        "             \"num_train_steps\": num_train_steps,\n",
        "             \"num_warmup_steps\": num_warmup_steps,\n",
        "             \"epoch\":NUM_TRAIN_EPOCHS})\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2K6Amv-9q4a",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW0qhSjIxhVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = estimator.predict(test_input_fn)\n",
        "prelabel=[]\n",
        "for pre in predictions:\n",
        "  prelabel.append(pre['labels'])\n",
        "print(classification_report(prelabel,list(data.iloc[(num_split+1):]['label'])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}